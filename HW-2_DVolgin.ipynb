{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 2\n",
    "\n",
    "**Дедлайн – 23:59 23.03.25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала повторим загрузку данных и вспомогательные фнукции из ДЗ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = 'mps'\n"
     ]
    }
   ],
   "source": [
    "import imagehash\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.transforms import v2 as vision_transforms_v2\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f'{device = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7390\n",
      "Classes count: 37\n",
      "Duplicate count: 74\n",
      "Deduplicated dataset size: 7316\n"
     ]
    }
   ],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    '''\n",
    "    https://www.kaggle.com/datasets/devdgohil/the-oxfordiiit-pet-dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, images_root):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_label = {}\n",
    "        self.label_to_class = []\n",
    "        for filename in os.listdir(images_root):\n",
    "            # .../<images_root>/Egyptian_Mau_167.jpg\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            self.image_paths.append(os.path.join(images_root, filename))\n",
    "            class_name = filename[:filename.rfind('_')]\n",
    "            if class_name not in self.class_to_label:\n",
    "                self.label_to_class.append(class_name)\n",
    "                self.class_to_label[class_name] = len(self.class_to_label)\n",
    "            self.labels.append(self.class_to_label[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "class CatBreedsDataset(Dataset):\n",
    "    '''\n",
    "    https://www.kaggle.com/datasets/ma7555/cat-breeds-dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, images_root):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_label = {}\n",
    "        self.label_to_class = []\n",
    "        for class_folder in os.listdir(images_root):\n",
    "            class_folder_path = os.path.join(images_root, class_folder)\n",
    "            if not os.path.isdir(class_folder_path):\n",
    "                continue\n",
    "            for filename in os.listdir(class_folder_path):\n",
    "                # .../<images_root>/Egyptian Mau/<...>.jpg\n",
    "                if not filename.endswith('.jpg'):\n",
    "                    continue\n",
    "                self.image_paths.append(os.path.join(images_root, class_folder, filename))\n",
    "                if class_folder not in self.class_to_label:\n",
    "                    self.label_to_class.append(class_folder)\n",
    "                    self.class_to_label[class_folder] = len(self.class_to_label)\n",
    "                self.labels.append(self.class_to_label[class_folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "class ImagehashDeduplicationDataset(Dataset):\n",
    "    def __init__(self, dataset, hash_func=imagehash.average_hash):\n",
    "        self.dataset = dataset\n",
    "        self.hash_func = hash_func\n",
    "        self.image_hash_to_idx = dict()\n",
    "        self.idx_is_duplicate_of = dict()\n",
    "        self.index_mapping = list()\n",
    "        for i in range(len(dataset)):\n",
    "            image_hash = hash_func(dataset[i]['image'])\n",
    "            if image_hash in self.image_hash_to_idx:\n",
    "                self.idx_is_duplicate_of[i] = self.image_hash_to_idx[image_hash]\n",
    "            else:\n",
    "                self.image_hash_to_idx[image_hash] = i\n",
    "                self.index_mapping.append(i)\n",
    "        self.labels = [self.dataset.labels[idx] for idx in self.index_mapping]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.index_mapping[idx]]\n",
    "\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        if hasattr(self.dataset, 'labels'):\n",
    "            self.labels = self.dataset.labels\n",
    "        else:\n",
    "            self.labels = [self.dataset[idx]['label'] for idx in range(len(self.dataset))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        item['image'] = self.transform(item['image'])\n",
    "        return item\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    return {'images': images, 'labels': labels}\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    vision_transforms_v2.ToImage(),\n",
    "\n",
    "    vision_transforms_v2.ToDtype(torch.uint8, scale=True),\n",
    "    vision_transforms_v2.Resize((256, 256), antialias=True),\n",
    "    vision_transforms_v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    vision_transforms_v2.RandomHorizontalFlip(p=0.5),\n",
    "    vision_transforms_v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "\n",
    "    vision_transforms_v2.ToDtype(torch.float32, scale=True),\n",
    "    vision_transforms_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    vision_transforms_v2.ToImage(),\n",
    "\n",
    "    vision_transforms_v2.ToDtype(torch.uint8, scale=True),\n",
    "    vision_transforms_v2.Resize((224, 224), antialias=True),\n",
    "\n",
    "    vision_transforms_v2.ToDtype(torch.float32, scale=True),\n",
    "    vision_transforms_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "main_dataset = OxfordPetDataset('./oxford-pet-dataset/images/')\n",
    "num_classes = len(set(main_dataset.labels))\n",
    "print(f'Dataset size: {len(main_dataset)}')\n",
    "print(f'Classes count: {num_classes}')\n",
    "\n",
    "deduplicated_main_dataset = ImagehashDeduplicationDataset(main_dataset)\n",
    "print(f'Duplicate count: {len(deduplicated_main_dataset.idx_is_duplicate_of)}')\n",
    "print(f'Deduplicated dataset size: {len(deduplicated_main_dataset)}')\n",
    "\n",
    "transformed_main_dataset = TransformedDataset(deduplicated_main_dataset, test_transforms)\n",
    "\n",
    "weak_dataset = CatBreedsDataset('./cat-breeds-dataset/images/')\n",
    "transformed_weak_dataset = TransformedDataset(weak_dataset, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 458/458 [00:37<00:00, 12.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_embeddings(dataset):\n",
    "    with torch.no_grad():\n",
    "        embedder = torchvision.models.resnet18(pretrained=True)\n",
    "        embedder.fc = nn.Identity()\n",
    "        embedder.to(device)\n",
    "\n",
    "        dl = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm.tqdm(dl):\n",
    "            embeds = embedder(batch['images'].to(device)).cpu().numpy()\n",
    "            for label, embed in zip(batch['labels'], embeds):\n",
    "                all_embeddings.append(embed)\n",
    "                all_labels.append(label.item())\n",
    "\n",
    "        return np.array(all_embeddings), np.array(all_labels)\n",
    "\n",
    "embeddings_array, _ = calculate_embeddings(transformed_main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_candidates(embeddings_array, dist_threshold):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_embeddings = scaler.fit_transform(embeddings_array)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute')\n",
    "    knn.fit(normalized_embeddings)\n",
    "\n",
    "    distances, indices = knn.kneighbors(normalized_embeddings)\n",
    "    duplicate_pairs = defaultdict(list)\n",
    "\n",
    "    for idx, (dists, neighbors) in enumerate(zip(distances, indices)):\n",
    "        for dist, neighbor_idx in zip(dists[1:], neighbors[1:]):\n",
    "            if dist < dist_threshold and idx < int(neighbor_idx):\n",
    "                duplicate_pairs[(idx, int(neighbor_idx))].append(float(dist))\n",
    "\n",
    "    return sorted(duplicate_pairs.items(), key=lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveIndicesDataset(Dataset):\n",
    "    def __init__(self, dataset, remove_indices):\n",
    "        self.dataset = dataset\n",
    "        self.remove_indices = remove_indices\n",
    "        self.index_mapping = list(set(range(len(dataset))) - set(remove_indices))\n",
    "        self.labels = [self.dataset.labels[idx] for idx in self.index_mapping]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[self.index_mapping[i]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "\n",
    "remove_indices = [p[1] for p, _ in get_duplicate_candidates(embeddings_array, 0.2)]\n",
    "\n",
    "filtered_main_dataset = RemoveIndicesDataset(\n",
    "    ImagehashDeduplicationDataset(\n",
    "        OxfordPetDataset('./oxford-pet-dataset/images/')\n",
    "    ),\n",
    "    remove_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470 1824\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(filtered_main_dataset)),\n",
    "    test_size=0.25,\n",
    "    stratify=filtered_main_dataset.labels,  # разбиваем классы равномерно\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TransformedDataset(\n",
    "    Subset(filtered_main_dataset, train_indices),\n",
    "    train_transforms\n",
    ")\n",
    "test_dataset = TransformedDataset(\n",
    "    Subset(filtered_main_dataset, test_indices),\n",
    "    test_transforms\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(true_labels, predicted_labels):\n",
    "    classes = sorted(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, predicted_labels),\n",
    "        'micro_f1': f1_score(true_labels, predicted_labels, average='micro', labels=classes),\n",
    "        'macro_f1': f1_score(true_labels, predicted_labels, average='macro', labels=classes),\n",
    "        'class_wise_precision': dict(zip(\n",
    "            map(int, classes),\n",
    "            map(float, precision_score(true_labels, predicted_labels, average=None, labels=classes))\n",
    "        )),\n",
    "        'micro_precision': precision_score(true_labels, predicted_labels, average='micro', labels=classes)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, n_iter, writer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = torch.tensor(0.0, device=device)\n",
    "    running_count = torch.tensor(0)\n",
    "\n",
    "    iterable = train_loader if writer is None else tqdm.tqdm(train_loader)\n",
    "\n",
    "    for batch in iterable:\n",
    "        inputs = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.detach() * inputs.size(0)\n",
    "        running_count += inputs.size(0)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train/loss', loss.detach(), n_iter)\n",
    "        n_iter += inputs.size(0)\n",
    "\n",
    "    return running_loss / running_count\n",
    "\n",
    "\n",
    "def validate(model, valid_loader, n_iter, writer):\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    iterable = valid_loader if writer is None else tqdm.tqdm(valid_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterable:\n",
    "            inputs = batch['images'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels.append(batch['labels'].numpy())\n",
    "            predictions.append(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    labels = np.concatenate(labels)\n",
    "    predictions = np.concatenate(predictions)\n",
    "\n",
    "    micro_acc = accuracy_score(labels, predictions)\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_precision = precision_score(labels, predictions, average='micro')\n",
    "\n",
    "    # print(f'Micro F1: {micro_f1:.4f}\\tMacro F1: {macro_f1:.4f}')\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('test/accuracy', micro_acc, n_iter)\n",
    "        writer.add_scalar('test/macro_f1', macro_f1, n_iter)\n",
    "        writer.add_scalar('test/micro_f1', micro_f1, n_iter)\n",
    "        writer.add_scalar('test/micro_precision', micro_precision, n_iter)\n",
    "        return {\n",
    "            'micro_acc': micro_acc,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_precision': micro_precision,\n",
    "        }\n",
    "    else:\n",
    "        return get_all_metrics(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Эксперименты с моделями без тюнинга гиперпараметров  (3 балл)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала попробуем обучить не голову resnet18, а полностью разморозить все веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter('./logs/resnet18-full_tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 1.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:09<00:00,  3.08it/s]\n",
      "100%|██████████| 86/86 [01:04<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 1.1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:09<00:00,  2.96it/s]\n",
      "100%|██████████| 86/86 [01:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 1.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.73it/s]\n",
      "100%|██████████| 86/86 [01:08<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 1.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "100%|██████████| 86/86 [01:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.75it/s]\n",
      "100%|██████████| 86/86 [01:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "100%|██████████| 86/86 [01:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.68it/s]\n",
      "100%|██████████| 86/86 [01:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.7594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.52it/s]\n",
      "100%|██████████| 86/86 [01:10<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.51it/s]\n",
      "100%|██████████| 86/86 [01:11<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За то же количество итераций получилось хуже качество, ещё и дольше училось.\n",
    "\n",
    "Здесь на изображении оранжевое - бейзлайн, файнтюн одного слоя; красное - кандидат, файнтюн всей модели.\n",
    "<img src=\"https://i.imgur.com/0YO6vws.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Теперь попробуем сначала претрейнить голову, затем немного дотюнить всю сетку с меньшим learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 86/86 [00:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 2.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.51it/s]\n",
      "100%|██████████| 86/86 [00:39<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 1.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.66it/s]\n",
      "100%|██████████| 86/86 [00:40<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.56it/s]\n",
      "100%|██████████| 86/86 [00:41<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.7584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.54it/s]\n",
      "100%|██████████| 86/86 [00:41<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.71it/s]\n",
      "100%|██████████| 86/86 [01:08<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.45it/s]\n",
      "100%|██████████| 86/86 [01:14<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:11<00:00,  2.42it/s]\n",
      "100%|██████████| 86/86 [01:14<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.68it/s]\n",
      "100%|██████████| 86/86 [01:05<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.50it/s]\n",
      "100%|██████████| 86/86 [00:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.3555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('./logs/resnet18-fc_then_full')\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5, 10):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход помог значимо улучшить качество.\n",
    "\n",
    "Здесь на изображении оранжевое - бейзлайн, файнтюн одного слоя; голубое - кандидат, последовательно файнтюн сначала последнего слоя, затем всей модели.\n",
    "<img src=\"https://i.imgur.com/YqBguK9.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Попробуем новую архитектуру - efficientnet_b3 (по размеру очень похожа на resnet18). Оставим те же гиперпараметры/сетап обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /Users/danwallgun/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n",
      "100%|██████████| 47.2M/47.2M [00:03<00:00, 13.4MB/s]\n",
      "100%|██████████| 86/86 [00:56<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 2.3710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:13<00:00,  2.16it/s]\n",
      "100%|██████████| 86/86 [00:48<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 1.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.40it/s]\n",
      "100%|██████████| 86/86 [00:48<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.9758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:13<00:00,  2.16it/s]\n",
      "100%|██████████| 86/86 [00:50<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:13<00:00,  2.18it/s]\n",
      "100%|██████████| 86/86 [00:52<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.7915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.29it/s]\n",
      "100%|██████████| 86/86 [02:19<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.38it/s]\n",
      "100%|██████████| 86/86 [02:09<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.3931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.40it/s]\n",
      "100%|██████████| 86/86 [02:09<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.3410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.40it/s]\n",
      "100%|██████████| 86/86 [02:10<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.38it/s]\n",
      "100%|██████████| 86/86 [02:09<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.2772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:12<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('./logs/effnetb3-fc_then_full')\n",
    "\n",
    "model = torchvision.models.efficientnet_b3(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5, 10):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficientnet, вопреки ожиданиям, не выбил лучшего качества (на imagenet метрики pretrained модели лучше resnet-ных), при этом учился сильно медленнее.\n",
    "\n",
    "Здесь на изображении голубое - бейзлайн, файнтюн resnet последовательно головы/всей сети; розовое - кандидат, efficientnet в том же сетапе.\n",
    "<img src=\"https://i.imgur.com/lizIxnX.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Теперь попробуем mobilenet_v3_large, он сильно эффективнее с точки зрения размера/компьюта, при этом с лучшим качеством на ImageNet согласно https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:39<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 2.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.28it/s]\n",
      "100%|██████████| 86/86 [00:35<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 1.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.52it/s]\n",
      "100%|██████████| 86/86 [00:32<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.8527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:07<00:00,  3.63it/s]\n",
      "100%|██████████| 86/86 [00:37<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.54it/s]\n",
      "100%|██████████| 86/86 [00:34<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.6978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.30it/s]\n",
      "100%|██████████| 86/86 [00:58<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.44it/s]\n",
      "100%|██████████| 86/86 [00:53<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.52it/s]\n",
      "100%|██████████| 86/86 [00:53<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.40it/s]\n",
      "100%|██████████| 86/86 [00:53<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.3111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:09<00:00,  3.12it/s]\n",
      "100%|██████████| 86/86 [00:57<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.2888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('./logs/mobnetv3-fc_then_full')\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_large(\n",
    "    weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5, 10):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось лучше, но не сильно)\n",
    "На графиках есть намёки на переобучение, интересно увеличить аугментации/добавить регуляризации.\n",
    "\n",
    "Здесь на изображении голубое - бейзлайн, файнтюн resnet последовательно головы/всей сети; розовое - кандидат, mobilenet_large в том же сетапе.\n",
    "<img src=\"https://i.imgur.com/oWoTl4j.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Попробуем взять модель значительно больше - convnext_tiny - 28.6M параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:06<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 1.4550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.62it/s]\n",
      "100%|██████████| 86/86 [01:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.4672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.63it/s]\n",
      "100%|██████████| 86/86 [01:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.3845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.63it/s]\n",
      "100%|██████████| 86/86 [01:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.62it/s]\n",
      "100%|██████████| 86/86 [01:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.61it/s]\n",
      "100%|██████████| 86/86 [04:18<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.62it/s]\n",
      "100%|██████████| 86/86 [04:14<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.2434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.65it/s]\n",
      "100%|██████████| 86/86 [04:12<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.62it/s]\n",
      "100%|██████████| 86/86 [04:15<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.1885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.62it/s]\n",
      "100%|██████████| 86/86 [04:15<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.1896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:17<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('./logs/convnexttiny-fc_then_full')\n",
    "\n",
    "model = torchvision.models.convnext_tiny(\n",
    "    weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5, 10):\n",
    "    n_iter = epoch * len(train_loader.dataset)\n",
    "    loss = train_epoch(model, criterion, optimizer, train_loader, n_iter, writer)\n",
    "    print(f'Epoch {epoch+1} train_loss: {loss:.4f}')\n",
    "    n_iter += len(train_loader.dataset)\n",
    "    validate(model, test_loader, n_iter, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ~2 раза большая модель действительно дала значительный буст к качеству.\n",
    "\n",
    "Здесь на изображении голубое - resnet, зеленое mobnet, белое - convnext в том же сетапе\n",
    "<img src=\"https://i.imgur.com/dAMKDl3.png\" style=\"width:90%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Эксперименты с моделями с тюнингом гиперпараметров  (4 балл)\n",
    "\n",
    "- выбрать не менее двух моделей из предыдущего пункта\n",
    "- подберите гиперпараметры (максимум 3 балла)\n",
    "    - использование Grid Search / Random Search для оптимизации одного гиперпараметра – 0.5 балла\n",
    "    - использование Grid Search / Random Search для оптимизации не менее двух гиперпараметров – 1 балл\n",
    "    - использование другого метода оптимизации гиперпараметров – 1 балл за каждый\n",
    "- при обучении трекать метрики с помощью wandb / clearml / mlflow или другого инструмента трекинга экспериментов (1 балл)\n",
    "- приложить скриншоты метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем mobnet и resnet модели для перебора гиперпараметров. Convnext тяжеловат и медленно учится.  \n",
    "\n",
    "Будем перебирать learning_rate до разморозки и после, а также weight_decay.\n",
    "\n",
    "Для простоты тестировать будем не на отдельном test сете, а на основном.  Чтобы ускорить процесс перебора, оставим только одну эпоху обучения головы и одну эпоху обучения всей сетки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mobnet(lr_head, lr_full, weight_decay, epochs_head=5, epochs_full=5):\n",
    "    model = torchvision.models.mobilenet_v3_large(\n",
    "        weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    "    )\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_head, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_full, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head, epochs_head + epochs_full):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    return validate(model, test_loader, n_iter, None)\n",
    "\n",
    "\n",
    "def train_resnet(lr_head, lr_full, weight_decay, epochs_head=5, epochs_full=5):\n",
    "    model = torchvision.models.resnet18(\n",
    "        weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_head, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_full, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head, epochs_head + epochs_full):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    return validate(model, test_loader, n_iter, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [03:18<13:10, 98.87s/it] /Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "100%|██████████| 10/10 [15:02<00:00, 90.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "    'lr_head': np.logspace(start=-5, stop=-2, num=10),\n",
    "    'lr_full': np.logspace(start=-5, stop=-2, num=10),\n",
    "    'weight_decay': np.linspace(start=0.0, stop=0.3, num=10),\n",
    "}\n",
    "\n",
    "hparam_writer = SummaryWriter('./logs/mobnet-hparam_tuning')\n",
    "\n",
    "for params in tqdm.tqdm(ParameterSampler(param_grid, n_iter=10, random_state=42)):\n",
    "    metrics = train_mobnet(**params, epochs_head=1, epochs_full=1)\n",
    "    hparam_writer.add_hparams(\n",
    "        params,\n",
    "        {\n",
    "            'hparam/accuracy': metrics['accuracy'],\n",
    "            'hparam/micro_f1': metrics['micro_f1'],\n",
    "            'hparam/macro_f1': metrics['macro_f1'],\n",
    "            'hparam/micro_precision': metrics['micro_precision'],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты перебора в tensorboard:\n",
    "\n",
    "Таблица\n",
    "<img src=\"https://i.imgur.com/nLHAxjB.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Параллельные координаты\n",
    "<img src=\"https://i.imgur.com/AyWZQ4q.png\" style=\"width:90%;\"/>\n",
    "\n",
    "\n",
    "Как мы видим, даже за малое количество эпох получаются очень хорошие результаты при правильно выборе гиперпараметров. Интуитивно, что в топе находятся эксперименты с большими learning_rate-ами, это позволяет сделать большой прогресс за малое число эпох, а верхняя граница перебора learning rate при этом выставлена достаточно адекватно (не очень большим), чтобы обучение все ещё нормально сходилось, а не прыгало большими шагами по весам.\n",
    "\n",
    "Попробуем сдвинуть перебираемый отрезок learning_rate-ов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " 10%|█         | 1/10 [01:34<14:06, 94.01s/it]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " 20%|██        | 2/10 [03:02<12:03, 90.47s/it]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " 30%|███       | 3/10 [04:24<10:08, 86.93s/it]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " 50%|█████     | 5/10 [07:12<07:05, 85.14s/it]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " 80%|████████  | 8/10 [12:11<03:12, 96.01s/it]/Users/danwallgun/ysda-workspaces/mlsd/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "100%|██████████| 10/10 [15:38<00:00, 93.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "    'lr_head': np.logspace(start=-4, stop=-1, num=10),\n",
    "    'lr_full': np.logspace(start=-4, stop=-1, num=10),\n",
    "    'weight_decay': np.linspace(start=0.0, stop=0.3, num=10),\n",
    "}\n",
    "\n",
    "hparam_writer = SummaryWriter('./logs/mobnet-hparam_tuning-2')\n",
    "\n",
    "for params in tqdm.tqdm(ParameterSampler(param_grid, n_iter=10, random_state=4242)):\n",
    "    metrics = train_mobnet(**params, epochs_head=1, epochs_full=1)\n",
    "    hparam_writer.add_hparams(\n",
    "        params,\n",
    "        {\n",
    "            'hparam/accuracy': metrics['accuracy'],\n",
    "            'hparam/micro_f1': metrics['micro_f1'],\n",
    "            'hparam/macro_f1': metrics['macro_f1'],\n",
    "            'hparam/micro_precision': metrics['micro_precision'],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим новые результаты в tensorboard:\n",
    "\n",
    "Параллельные координаты\n",
    "<img src=\"https://i.imgur.com/qTWDFba.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Подтверждаем интуицию, что для обучения головы нужен достаточно большой lr (видимо, оптимум приближается к, но не выше ~0.002). А для тюна полной модели нужен достаточно маленький, чтобы не портить хорошие pretrained фичи (оптимум лежит в [1e-5, 2e-4]).  \n",
    "Weight decay при этом не сильно влияет на качество - ему достаточно быть не \"совсем жёстким\" (в нашем переборе <= 0.16).\n",
    "\n",
    "Подвинем ещё границы перебора с обновлёнными знаниями, в этот раз переберём resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:19<00:00, 67.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "    'lr_head': np.logspace(start=-3.5, stop=-2.5, num=10),\n",
    "    'lr_full': np.logspace(start=-5, stop=-3.5, num=10),\n",
    "    'weight_decay': np.linspace(start=0.0, stop=0.2, num=10),\n",
    "}\n",
    "\n",
    "hparam_writer = SummaryWriter('./logs/resnet-hparam_tuning-3')\n",
    "\n",
    "for params in tqdm.tqdm(ParameterSampler(param_grid, n_iter=10, random_state=424242)):\n",
    "    metrics = train_resnet(**params, epochs_head=1, epochs_full=1)\n",
    "    hparam_writer.add_hparams(\n",
    "        params,\n",
    "        {\n",
    "            'hparam/accuracy': metrics['accuracy'],\n",
    "            'hparam/micro_f1': metrics['micro_f1'],\n",
    "            'hparam/macro_f1': metrics['macro_f1'],\n",
    "            'hparam/micro_precision': metrics['micro_precision'],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полные результаты отфильтровал по macro_f1 >= 0.75\n",
    "\n",
    "Параллельные координаты  \n",
    "<img src=\"https://i.imgur.com/IGVzdPr.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Таблица  \n",
    "<img src=\"https://i.imgur.com/Dl41fir.png\" style=\"width:90%;\"/>\n",
    "\n",
    "Как мы видим, все resnet-запуски оказались с около-топовыми метриками (0.8+). Значит, заданные интервалы оптимума параметров выбраны хорошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить convnext с топ1 гиперпараметрами.\n",
    "\n",
    "lr_head=0.0021544, lr_full=0.000046416, weight_decay=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics[\"macro_f1\"] = 0.9453548278400434\tmetrics[\"micro_precision\"] = 0.9457236842105263\n"
     ]
    }
   ],
   "source": [
    "def train_convnext(lr_head, lr_full, weight_decay, epochs_head=5, epochs_full=5):\n",
    "    model = torchvision.models.convnext_tiny(\n",
    "        weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_head, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_full, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs_head, epochs_head + epochs_full):\n",
    "        train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "    return validate(model, test_loader, n_iter, None)\n",
    "\n",
    "metrics = train_convnext(lr_head=0.0021544, lr_full=0.000046416, weight_decay=0.0, epochs_head=1, epochs_full=1)\n",
    "print(f'{metrics[\"macro_f1\"] = }\\t{metrics[\"micro_precision\"] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились примерно такие же метрики, как с моими дефолтными гиперами, но в 5 раз меньше эпох.\n",
    "\n",
    "Потенциально можно выбить ещё больше качества, если учить больше эпох (в том числе можно дополнительно подобрать гиперпараметры). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Итоговый выбор модели (1 балл)\n",
    "\n",
    "- Проанализировать результаты обучения (0.5 балла)\n",
    "- Выбрать модель, которая пойдёт \"в прод\", с обоснованием (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результаты обучений проанализированы рядом с соответствующими обучениями\n",
    "- Для выкатки в прод\n",
    "    - я бы выбрал ConvNext, если ограничения по ресурсам/скорости позволяют, т.к. она даёт заметно лучшее качество\n",
    "    - однако если требования к компактности/скорости/latency более жесткие, я бы выбрал mobilenet, т.к. она даёт лучшее качество из небольших эффективных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Демо инференса модели  (2 балл)\n",
    "\n",
    "- сделать простейший интерфейс для демо инференса модели (1 балл)\n",
    "- подготовить и описать способ быстро запустить демо (1 балл)\n",
    "    - необходимо выполнить более 2 действий для запуска демо – 0.5 балла\n",
    "    - подготовлен скрипт, который позволяет запустить демо за 1 действие – 0.5 балла\n",
    "    - запуск демо должен быть воспроизводимым на других устройствах (под управлением linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics[\"macro_f1\"] = 0.9002829633133199\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.mobilenet_v3_large(\n",
    "    weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0021544, weight_decay=0.0)\n",
    "for epoch in range(1):\n",
    "    train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.000046416, weight_decay=0.0)\n",
    "for epoch in range(1, 2):\n",
    "    train_epoch(model, criterion, optimizer, train_loader, n_iter, None)\n",
    "\n",
    "metrics = validate(model, test_loader, n_iter, None)\n",
    "print(f'{metrics[\"macro_f1\"] = }')\n",
    "torch.save(model, 'production-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_to_class.json', 'w') as f:\n",
    "    json.dump(main_dataset.label_to_class, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting web.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile web.py\n",
    "\n",
    "import io\n",
    "import json\n",
    "import torch\n",
    "import gradio as gr\n",
    "from torchvision.transforms import v2 as vision_transforms_v2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_transforms():\n",
    "    return vision_transforms_v2.Compose([\n",
    "        vision_transforms_v2.ToImage(),\n",
    "\n",
    "        vision_transforms_v2.ToDtype(torch.uint8, scale=True),\n",
    "        vision_transforms_v2.Resize((224, 224), antialias=True),\n",
    "\n",
    "        vision_transforms_v2.ToDtype(torch.float32, scale=True),\n",
    "        vision_transforms_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_class_names(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    model = torch.load(filename, weights_only=False, map_location='cpu')\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_prediction(image, transforms, model, label_to_class):\n",
    "    image_input = transforms(image).unsqueeze(0)\n",
    "    outputs = model(image_input)\n",
    "    predicted = outputs.argmax(dim=1).cpu().numpy()\n",
    "    label = predicted.item()\n",
    "    return label, label_to_class[label]\n",
    "\n",
    "\n",
    "def main():\n",
    "    label_to_class = load_class_names('label_to_class.json')\n",
    "    model = load_model('production-model.pt')\n",
    "    transforms = load_transforms()\n",
    "\n",
    "    fn = lambda image: get_prediction(image, transforms, model, label_to_class)\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=fn,\n",
    "        inputs=gr.Image(type=\"pil\"),\n",
    "        outputs=[\n",
    "            gr.Number(label=\"Class ID\"),\n",
    "            gr.Textbox(label=\"Class Name\")\n",
    "        ],\n",
    "        title=\"Image Classification Demo\",\n",
    "        description=\"Upload an image to classify it using a trained model\",\n",
    "    )\n",
    "    interface.launch(share=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы запустить Web демонстрацию с интерфейсом, нужен один скрипт web.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a871fbedf17807ed27.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://a871fbedf17807ed27.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!python web.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит это так:\n",
    "\n",
    "<img src=\"https://i.imgur.com/PZHI8w0.png\" style=\"width:90%;\"/>\n",
    "\n",
    "(кстати, фотографии не было в train и ответ модели верный)  \n",
    "(черипик, получается :D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
